{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199bb195-c8ea-45b7-a46e-bc45982a4df3",
   "metadata": {},
   "source": [
    "# Phase 3: Memory Augmented RAG on nuPlan\n",
    "**Goal:** \n",
    "- Build a continual memory on top of the static FAISS index from Phase 2\n",
    "- Use a small LLM (GPT-2) to produce high level driving decisions\n",
    "- Log interactions so Phase 4 can evaluate performance\n",
    "\n",
    "**Team:** Karina Shah, Dhruvina Gujarati, Nilay Kumar, Nishanth Krishna Churchmal\n",
    "\n",
    "**Course:** CSE 475 - Fall 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6aa34f-2afd-4c3b-ac1c-012a653b3f53",
   "metadata": {},
   "source": [
    "Phase 3: Memory-Augmented RAG Planner\n",
    "\n",
    "We start from the FAISS index + metadata built in Phase 2 (static scenario memory).\n",
    "\n",
    "We create a second FAISS index (index_memory) that starts empty and stores newly encountered scenarios.\n",
    "\n",
    "For each query (text description of a nuPlan scenario):\n",
    "\n",
    "Encode the query with SentenceTransformer into an embedding.\n",
    "\n",
    "Retrieve nearest neighbors from both the static index and memory index.\n",
    "\n",
    "Concatenate and sort by similarity, tagging rows as source=static or source=memory.\n",
    "\n",
    "Format the top-k retrieved scenarios into a prompt and pass them to GPT-2, which generates a 2–3 sentence high-level driving plan.\n",
    "\n",
    "Log this new “experience” into the memory index so future queries can retrieve it.\n",
    "\n",
    "We show that over multiple queries, the memory index grows and starts to dominate retrieval for similar situations (see source=memory in the top retrieved rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fddd44-61fc-41f6-ab17-80ebf9e91d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Paths consistent with Phase 2\n",
    "NUPLAN_DATA_ROOT = Path(os.environ[\"NUPLAN_DATA_ROOT\"])\n",
    "NUPLAN_EXP_ROOT = Path(os.environ[\"NUPLAN_EXP_ROOT\"])\n",
    "\n",
    "INDEX_DIR = NUPLAN_EXP_ROOT / \"rag_index\"\n",
    "INDEX_PATH = INDEX_DIR / \"faiss_index.bin\"\n",
    "METADATA_PATH = INDEX_DIR / \"metadata.parquet\"\n",
    "\n",
    "# New: memory-augmented paths\n",
    "MEMORY_INDEX_PATH = INDEX_DIR / \"faiss_index_memory.bin\"\n",
    "MEMORY_METADATA_PATH = INDEX_DIR / \"metadata_memory.parquet\"\n",
    "\n",
    "print(\"INDEX_DIR:\", INDEX_DIR)\n",
    "print(\"INDEX_PATH:\", INDEX_PATH)\n",
    "print(\"METADATA_PATH:\", METADATA_PATH)\n",
    "print(\"MEMORY_INDEX_PATH:\", MEMORY_INDEX_PATH)\n",
    "print(\"MEMORY_METADATA_PATH:\", MEMORY_METADATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381f573-d017-495c-919f-da5b8c2912f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static FAISS index and metadata from Phase 2\n",
    "print(\"Loading static FAISS index and metadata...\")\n",
    "\n",
    "index_static = faiss.read_index(str(INDEX_PATH))\n",
    "metadata_static = pd.read_parquet(METADATA_PATH, engine=\"fastparquet\")\n",
    "\n",
    "print(\"Static index size:\", index_static.ntotal)\n",
    "print(\"Metadata rows:\", len(metadata_static))\n",
    "\n",
    "# Load the same embedding model used in Phase 2\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "print(\"Embedding model loaded:\", embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565168e5-f914-4ec5-976a-e19440c4822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize or load memory index + metadata\n",
    "\n",
    "def init_or_load_memory_index(static_index: faiss.Index):\n",
    "    d = static_index.d  # embedding dimension\n",
    "\n",
    "    if MEMORY_INDEX_PATH.exists() and MEMORY_METADATA_PATH.exists():\n",
    "        print(\"Loading existing memory index...\")\n",
    "        index_mem = faiss.read_index(str(MEMORY_INDEX_PATH))\n",
    "        metadata_mem = pd.read_parquet(MEMORY_METADATA_PATH, engine=\"pyarrow\")\n",
    "    else:\n",
    "        print(\"No memory index found. Creating an empty one...\")\n",
    "        index_mem = faiss.IndexFlatIP(d)\n",
    "        metadata_mem = pd.DataFrame(\n",
    "            columns=[\"scenario_id\", \"scenario_type\", \"lidar_pc_token\", \"text\", \"source\"]\n",
    "        )\n",
    "\n",
    "    print(\"Memory index size:\", index_mem.ntotal)\n",
    "    print(\"Memory metadata rows:\", len(metadata_mem))\n",
    "    return index_mem, metadata_mem\n",
    "\n",
    "\n",
    "index_memory, metadata_memory = init_or_load_memory_index(index_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa35b1d-1bd9-4b57-8149-ee9d77e8de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"Embed a text query into the same vector space as scenarios.\"\"\"\n",
    "    q_emb = embed_model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "    return q_emb\n",
    "\n",
    "\n",
    "def search_static(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve from the fixed Phase 2 index.\"\"\"\n",
    "    q_emb = embed_query(query)\n",
    "    scores, idxs = index_static.search(q_emb, k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    results = metadata_static.iloc[idxs].copy()\n",
    "    results[\"score\"] = scores\n",
    "    results[\"source\"] = \"static\"\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_memory(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve from the growing memory index. If empty, return empty df.\"\"\"\n",
    "    if index_memory.ntotal == 0:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"scenario_id\", \"scenario_type\", \"lidar_pc_token\", \"text\", \"score\", \"source\"]\n",
    "        )\n",
    "\n",
    "    q_emb = embed_query(query)\n",
    "    scores, idxs = index_memory.search(q_emb, k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    results = metadata_memory.iloc[idxs].copy()\n",
    "    results[\"score\"] = scores\n",
    "    results[\"source\"] = \"memory\"\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_with_memory(query: str, k_static: int = 5, k_memory: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Combine results from static and memory indexes.\"\"\"\n",
    "    static_results = search_static(query, k=k_static)\n",
    "    memory_results = search_memory(query, k=k_memory)\n",
    "\n",
    "    combined = pd.concat([static_results, memory_results], ignore_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    combined = combined.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69be1f-c266-4c9b-ad64-3cbd38e46f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_with_memory(\"hard braking scenario\", k_static=3, k_memory=3).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd536b8b-edb3-48b5-a81a-df41871ef25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small causal LLM (GPT-2)\n",
    "llm_name = \"gpt2\"  # could also use \"distilgpt2\" if you want even smaller\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(llm_name)\n",
    "\n",
    "# GPT-2 has no pad token by default; set pad = eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_llm.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_llm = model_llm.to(device)\n",
    "\n",
    "print(\"Loaded LLM:\", llm_name, \"on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374026b3-bc4e-4f13-88f2-3a50ae918274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_block(retrieved: pd.DataFrame, top_m: int = 3) -> str:\n",
    "    \"\"\"Format top-m retrieved rows as a context string for the LLM.\"\"\"\n",
    "    rows = retrieved.head(top_m)\n",
    "    lines = []\n",
    "    for i, row in rows.iterrows():\n",
    "        line = (\n",
    "            f\"{i+1}. [source={row['source']}] \"\n",
    "            f\"type={row['scenario_type']} | text={row['text']}\"\n",
    "        )\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def generate_plan(query: str, retrieved: pd.DataFrame, top_m: int = 3) -> str:\n",
    "    \"\"\"Use GPT-2 to generate a high-level driving plan.\"\"\"\n",
    "    context = build_context_block(retrieved, top_m=top_m)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an autonomous driving planner that reasons over past scenarios.\\n\\n\"\n",
    "        \"Retrieved past scenarios:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Current query / situation:\\n{query}\\n\\n\"\n",
    "        \"Based on the retrieved scenarios, describe a safe high-level plan \"\n",
    "        \"for the ego vehicle in 2-3 sentences.\\n\\n\"\n",
    "        \"Plan:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return only the part after \"Plan:\"\n",
    "    if \"Plan:\" in generated:\n",
    "        return generated.split(\"Plan:\", 1)[-1].strip()\n",
    "    else:\n",
    "        return generated.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8c7a2-7b1b-4b26-8024-40bde13ad670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory_entry(\n",
    "    query: str,\n",
    "    plan: str,\n",
    "    top_retrieved: pd.Series,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a memory text combining query, plan, and retrieved scenario metadata.\n",
    "    Append to memory index + metadata and save to disk.\n",
    "    \"\"\"\n",
    "    global index_memory, metadata_memory\n",
    "\n",
    "    memory_text = (\n",
    "        f\"Memory note | query: {query} | \"\n",
    "        f\"plan: {plan} | \"\n",
    "        f\"base_scenario_type: {top_retrieved['scenario_type']} | \"\n",
    "        f\"base_scenario_id: {top_retrieved['scenario_id']}\"\n",
    "    )\n",
    "\n",
    "    # Embed and add to FAISS\n",
    "    emb = embed_model.encode(\n",
    "        [memory_text],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    index_memory.add(emb)\n",
    "\n",
    "    new_row = {\n",
    "        \"scenario_id\": str(top_retrieved[\"scenario_id\"]),\n",
    "        \"scenario_type\": str(top_retrieved[\"scenario_type\"]),\n",
    "        \"lidar_pc_token\": str(top_retrieved[\"lidar_pc_token\"]),\n",
    "        \"text\": memory_text,\n",
    "        \"source\": \"memory\",\n",
    "    }\n",
    "    metadata_memory = pd.concat(\n",
    "        [metadata_memory, pd.DataFrame([new_row])],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # Persist to disk\n",
    "    faiss.write_index(index_memory, str(MEMORY_INDEX_PATH))\n",
    "    metadata_memory.to_parquet(MEMORY_METADATA_PATH, index=False, engine=\"fastparquet\")\n",
    "\n",
    "    print(\"✅ Memory updated. New memory index size:\", index_memory.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817217b3-8c5b-486f-8c10-646ad9c26fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_with_memory(\n",
    "    query: str,\n",
    "    k_static: int = 5,\n",
    "    k_memory: int = 5,\n",
    "    top_m_for_llm: int = 3,\n",
    "    update_memory_flag: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full Phase 3 pipeline:\n",
    "    1. Retrieve from static + memory indexes\n",
    "    2. Generate a high-level plan with GPT-2\n",
    "    3. Optionally update memory with this new experience\n",
    "    \"\"\"\n",
    "    # 1) Retrieval\n",
    "    retrieved = search_with_memory(query, k_static=k_static, k_memory=k_memory)\n",
    "    if retrieved.empty:\n",
    "        print(\"No retrieved scenarios. Something is wrong with the index.\")\n",
    "        return None\n",
    "\n",
    "    # 2) Use LLM to generate plan\n",
    "    plan = generate_plan(query, retrieved, top_m=top_m_for_llm)\n",
    "\n",
    "    # 3) Optional memory update (use the single best retrieved item as anchor)\n",
    "    if update_memory_flag:\n",
    "        top_row = retrieved.iloc[0]\n",
    "        add_memory_entry(query, plan, top_row)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved\": retrieved,\n",
    "        \"plan\": plan,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b772e-de34-49ee-aafa-47ebc2f7c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries = [\n",
    "    \"The ego vehicle is approaching a red light with a slow lead car ahead.\",\n",
    "    \"The ego vehicle is entering a roundabout with multiple vehicles already inside.\",\n",
    "    \"A pedestrian starts crossing unexpectedly at a crosswalk while the ego car is braking.\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for q in example_queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"QUERY:\", q)\n",
    "    out = plan_with_memory(q, k_static=5, k_memory=5, top_m_for_llm=3, update_memory_flag=True)\n",
    "    results.append(out)\n",
    "    print(\"\\nGenerated plan:\\n\", out[\"plan\"])\n",
    "    print(\"\\nTop retrieved sources:\\n\", out[\"retrieved\"][[\"scenario_type\", \"source\", \"score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1f809-c88a-458c-bf9e-7243069375b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
