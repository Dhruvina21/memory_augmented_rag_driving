{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199bb195-c8ea-45b7-a46e-bc45982a4df3",
   "metadata": {},
   "source": [
    "# Phase 3: Memory Augmented RAG on nuPlan\n",
    "**Goal:** \n",
    "- Build a continual memory on top of the static FAISS index from Phase 2\n",
    "- Use a small LLM (GPT-2) to produce high level driving decisions\n",
    "- Log interactions so Phase 4 can evaluate performance\n",
    "\n",
    "**Team:** Karina Shah, Dhruvina Gujarati, Nilay Kumar, Nishanth Krishna Churchmal\n",
    "\n",
    "**Course:** CSE 475 - Fall 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6aa34f-2afd-4c3b-ac1c-012a653b3f53",
   "metadata": {},
   "source": [
    "Phase 3: Memory-Augmented RAG Planner\n",
    "\n",
    "We start from the FAISS index + metadata built in Phase 2 (static scenario memory).\n",
    "\n",
    "We create a second FAISS index (index_memory) that starts empty and stores newly encountered scenarios.\n",
    "\n",
    "For each query (text description of a nuPlan scenario):\n",
    "\n",
    "Encode the query with SentenceTransformer into an embedding.\n",
    "\n",
    "Retrieve nearest neighbors from both the static index and memory index.\n",
    "\n",
    "Concatenate and sort by similarity, tagging rows as source=static or source=memory.\n",
    "\n",
    "Format the top-k retrieved scenarios into a prompt and pass them to GPT-2, which generates a 2–3 sentence high-level driving plan.\n",
    "\n",
    "Log this new “experience” into the memory index so future queries can retrieve it.\n",
    "\n",
    "We show that over multiple queries, the memory index grows and starts to dominate retrieval for similar situations (see source=memory in the top retrieved rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66fddd44-61fc-41f6-ab17-80ebf9e91d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilayjkumar/miniconda3/envs/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "INDEX_DIR: /home/nilayjkumar/nuplan/exp/rag_index\n",
      "INDEX_PATH: /home/nilayjkumar/nuplan/exp/rag_index/faiss_index.bin\n",
      "METADATA_PATH: /home/nilayjkumar/nuplan/exp/rag_index/metadata.parquet\n",
      "MEMORY_INDEX_PATH: /home/nilayjkumar/nuplan/exp/rag_index/faiss_index_memory.bin\n",
      "MEMORY_METADATA_PATH: /home/nilayjkumar/nuplan/exp/rag_index/metadata_memory.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# Paths consistent with Phase 2\n",
    "NUPLAN_DATA_ROOT = Path(os.environ[\"NUPLAN_DATA_ROOT\"])\n",
    "NUPLAN_EXP_ROOT = Path(os.environ[\"NUPLAN_EXP_ROOT\"])\n",
    "\n",
    "INDEX_DIR = NUPLAN_EXP_ROOT / \"rag_index\"\n",
    "INDEX_PATH = INDEX_DIR / \"faiss_index.bin\"\n",
    "METADATA_PATH = INDEX_DIR / \"metadata.parquet\"\n",
    "\n",
    "# New: memory-augmented paths\n",
    "MEMORY_INDEX_PATH = INDEX_DIR / \"faiss_index_memory.bin\"\n",
    "MEMORY_METADATA_PATH = INDEX_DIR / \"metadata_memory.parquet\"\n",
    "\n",
    "print(\"INDEX_DIR:\", INDEX_DIR)\n",
    "print(\"INDEX_PATH:\", INDEX_PATH)\n",
    "print(\"METADATA_PATH:\", METADATA_PATH)\n",
    "print(\"MEMORY_INDEX_PATH:\", MEMORY_INDEX_PATH)\n",
    "print(\"MEMORY_METADATA_PATH:\", MEMORY_METADATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e381f573-d017-495c-919f-da5b8c2912f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading static FAISS index and metadata...\n",
      "Static index size: 13812\n",
      "Metadata rows: 13812\n",
      "Embedding model loaded: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Load static FAISS index and metadata from Phase 2\n",
    "print(\"Loading static FAISS index and metadata...\")\n",
    "\n",
    "index_static = faiss.read_index(str(INDEX_PATH))\n",
    "metadata_static = pd.read_parquet(METADATA_PATH, engine=\"fastparquet\")\n",
    "\n",
    "print(\"Static index size:\", index_static.ntotal)\n",
    "print(\"Metadata rows:\", len(metadata_static))\n",
    "\n",
    "# Load the same embedding model used in Phase 2\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "print(\"Embedding model loaded:\", embedding_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565168e5-f914-4ec5-976a-e19440c4822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No memory index found. Creating an empty one...\n",
      "Memory index size: 0\n",
      "Memory metadata rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize or load memory index + metadata\n",
    "\n",
    "def init_or_load_memory_index(static_index: faiss.Index):\n",
    "    d = static_index.d  # embedding dimension\n",
    "\n",
    "    if MEMORY_INDEX_PATH.exists() and MEMORY_METADATA_PATH.exists():\n",
    "        print(\"Loading existing memory index...\")\n",
    "        index_mem = faiss.read_index(str(MEMORY_INDEX_PATH))\n",
    "        metadata_mem = pd.read_parquet(MEMORY_METADATA_PATH, engine=\"fastparquet\")\n",
    "    else:\n",
    "        print(\"No memory index found. Creating an empty one...\")\n",
    "        index_mem = faiss.IndexFlatIP(d)\n",
    "        metadata_mem = pd.DataFrame(\n",
    "            columns=[\"scenario_id\", \"scenario_type\", \"lidar_pc_token\", \"text\", \"source\"]\n",
    "        )\n",
    "\n",
    "    print(\"Memory index size:\", index_mem.ntotal)\n",
    "    print(\"Memory metadata rows:\", len(metadata_mem))\n",
    "    return index_mem, metadata_mem\n",
    "\n",
    "\n",
    "index_memory, metadata_memory = init_or_load_memory_index(index_static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa35b1d-1bd9-4b57-8149-ee9d77e8de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> np.ndarray:\n",
    "    \"\"\"Embed a text query into the same vector space as scenarios.\"\"\"\n",
    "    q_emb = embed_model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "    return q_emb\n",
    "\n",
    "\n",
    "def search_static(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve from the fixed Phase 2 index.\"\"\"\n",
    "    q_emb = embed_query(query)\n",
    "    scores, idxs = index_static.search(q_emb, k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    results = metadata_static.iloc[idxs].copy()\n",
    "    results[\"score\"] = scores\n",
    "    results[\"source\"] = \"static\"\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_memory(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve from the growing memory index. If empty, return empty df.\"\"\"\n",
    "    if index_memory.ntotal == 0:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"scenario_id\", \"scenario_type\", \"lidar_pc_token\", \"text\", \"score\", \"source\"]\n",
    "        )\n",
    "\n",
    "    q_emb = embed_query(query)\n",
    "    scores, idxs = index_memory.search(q_emb, k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    results = metadata_memory.iloc[idxs].copy()\n",
    "    results[\"score\"] = scores\n",
    "    results[\"source\"] = \"memory\"\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_with_memory(query: str, k_static: int = 5, k_memory: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Combine results from static and memory indexes.\"\"\"\n",
    "    static_results = search_static(query, k=k_static)\n",
    "    memory_results = search_memory(query, k=k_memory)\n",
    "\n",
    "    combined = pd.concat([static_results, memory_results], ignore_index=True)\n",
    "\n",
    "    # Sort by score descending\n",
    "    combined = combined.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d69be1f-c266-4c9b-ad64-3cbd38e46f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1710/230957889.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([static_results, memory_results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>scenario_type</th>\n",
       "      <th>lidar_pc_token</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2a6d704894b6565e</td>\n",
       "      <td>stationary_in_traffic</td>\n",
       "      <td>d5883187cabe585c</td>\n",
       "      <td>Scenario type: stationary_in_traffic | Lidar t...</td>\n",
       "      <td>0.260185</td>\n",
       "      <td>static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a93cfb1ba7ab5b6c</td>\n",
       "      <td>stationary_in_traffic</td>\n",
       "      <td>941adc13b541550b</td>\n",
       "      <td>Scenario type: stationary_in_traffic | Lidar t...</td>\n",
       "      <td>0.238049</td>\n",
       "      <td>static</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7fa49b3c49fd5e6d</td>\n",
       "      <td>stationary_in_traffic</td>\n",
       "      <td>e1c741a273ff59b1</td>\n",
       "      <td>Scenario type: stationary_in_traffic | Lidar t...</td>\n",
       "      <td>0.233310</td>\n",
       "      <td>static</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        scenario_id          scenario_type    lidar_pc_token  \\\n",
       "0  2a6d704894b6565e  stationary_in_traffic  d5883187cabe585c   \n",
       "1  a93cfb1ba7ab5b6c  stationary_in_traffic  941adc13b541550b   \n",
       "2  7fa49b3c49fd5e6d  stationary_in_traffic  e1c741a273ff59b1   \n",
       "\n",
       "                                                text     score  source  \n",
       "0  Scenario type: stationary_in_traffic | Lidar t...  0.260185  static  \n",
       "1  Scenario type: stationary_in_traffic | Lidar t...  0.238049  static  \n",
       "2  Scenario type: stationary_in_traffic | Lidar t...  0.233310  static  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_with_memory(\"hard braking scenario\", k_static=3, k_memory=3).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd536b8b-edb3-48b5-a81a-df41871ef25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM: gpt2 on cpu\n"
     ]
    }
   ],
   "source": [
    "# Load a small causal LLM (GPT-2)\n",
    "llm_name = \"gpt2\"  # could also use \"distilgpt2\" if you want even smaller\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(llm_name)\n",
    "\n",
    "# GPT-2 has no pad token by default; set pad = eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_llm.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_llm = model_llm.to(device)\n",
    "\n",
    "print(\"Loaded LLM:\", llm_name, \"on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "374026b3-bc4e-4f13-88f2-3a50ae918274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_block(retrieved: pd.DataFrame, top_m: int = 3) -> str:\n",
    "    \"\"\"Format top-m retrieved rows as a context string for the LLM.\"\"\"\n",
    "    rows = retrieved.head(top_m)\n",
    "    lines = []\n",
    "    for i, row in rows.iterrows():\n",
    "        line = (\n",
    "            f\"{i+1}. [source={row['source']}] \"\n",
    "            f\"type={row['scenario_type']} | text={row['text']}\"\n",
    "        )\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def generate_plan(query: str, retrieved: pd.DataFrame, top_m: int = 3) -> str:\n",
    "    \"\"\"Use GPT-2 to generate a high-level driving plan.\"\"\"\n",
    "    context = build_context_block(retrieved, top_m=top_m)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an autonomous driving planner that reasons over past scenarios.\\n\\n\"\n",
    "        \"Retrieved past scenarios:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Current query / situation:\\n{query}\\n\\n\"\n",
    "        \"Based on the retrieved scenarios, describe a safe high-level plan \"\n",
    "        \"for the ego vehicle in 2-3 sentences.\\n\\n\"\n",
    "        \"Plan:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return only the part after \"Plan:\"\n",
    "    if \"Plan:\" in generated:\n",
    "        return generated.split(\"Plan:\", 1)[-1].strip()\n",
    "    else:\n",
    "        return generated.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a8c7a2-7b1b-4b26-8024-40bde13ad670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_memory_entry(\n",
    "    query: str,\n",
    "    plan: str,\n",
    "    top_retrieved: pd.Series,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a memory text combining query, plan, and retrieved scenario metadata.\n",
    "    Append to memory index + metadata and save to disk.\n",
    "    \"\"\"\n",
    "    global index_memory, metadata_memory\n",
    "\n",
    "    memory_text = (\n",
    "        f\"Memory note | query: {query} | \"\n",
    "        f\"plan: {plan} | \"\n",
    "        f\"base_scenario_type: {top_retrieved['scenario_type']} | \"\n",
    "        f\"base_scenario_id: {top_retrieved['scenario_id']}\"\n",
    "    )\n",
    "\n",
    "    # Embed and add to FAISS\n",
    "    emb = embed_model.encode(\n",
    "        [memory_text],\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    index_memory.add(emb)\n",
    "\n",
    "    new_row = {\n",
    "        \"scenario_id\": str(top_retrieved[\"scenario_id\"]),\n",
    "        \"scenario_type\": str(top_retrieved[\"scenario_type\"]),\n",
    "        \"lidar_pc_token\": str(top_retrieved[\"lidar_pc_token\"]),\n",
    "        \"text\": memory_text,\n",
    "        \"source\": \"memory\",\n",
    "    }\n",
    "    metadata_memory = pd.concat(\n",
    "        [metadata_memory, pd.DataFrame([new_row])],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # Persist to disk\n",
    "    faiss.write_index(index_memory, str(MEMORY_INDEX_PATH))\n",
    "    metadata_memory.to_parquet(MEMORY_METADATA_PATH, index=False, engine=\"fastparquet\")\n",
    "\n",
    "    print(\"✅ Memory updated. New memory index size:\", index_memory.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "817217b3-8c5b-486f-8c10-646ad9c26fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_with_memory(\n",
    "    query: str,\n",
    "    k_static: int = 5,\n",
    "    k_memory: int = 5,\n",
    "    top_m_for_llm: int = 3,\n",
    "    update_memory_flag: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full Phase 3 pipeline:\n",
    "    1. Retrieve from static + memory indexes\n",
    "    2. Generate a high-level plan with GPT-2\n",
    "    3. Optionally update memory with this new experience\n",
    "    \"\"\"\n",
    "    # 1) Retrieval\n",
    "    retrieved = search_with_memory(query, k_static=k_static, k_memory=k_memory)\n",
    "    if retrieved.empty:\n",
    "        print(\"No retrieved scenarios. Something is wrong with the index.\")\n",
    "        return None\n",
    "\n",
    "    # 2) Use LLM to generate plan\n",
    "    plan = generate_plan(query, retrieved, top_m=top_m_for_llm)\n",
    "\n",
    "    # 3) Optional memory update (use the single best retrieved item as anchor)\n",
    "    if update_memory_flag:\n",
    "        top_row = retrieved.iloc[0]\n",
    "        add_memory_entry(query, plan, top_row)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved\": retrieved,\n",
    "        \"plan\": plan,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163b772e-de34-49ee-aafa-47ebc2f7c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1710/230957889.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([static_results, memory_results], ignore_index=True)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: The ego vehicle is approaching a red light with a slow lead car ahead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory updated. New memory index size: 1\n",
      "\n",
      "Generated plan:\n",
      " In the future, the vehicle will be slow to the right or left due to a small amount of traffic ahead.\n",
      "\n",
      "Plan: The vehicle will follow a green light. The vehicle will start to turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will continue to turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Top retrieved sources:\n",
      "              scenario_type  source     score\n",
      "0  near_high_speed_vehicle  static  0.272152\n",
      "1  near_high_speed_vehicle  static  0.271030\n",
      "2  near_high_speed_vehicle  static  0.269611\n",
      "3  near_high_speed_vehicle  static  0.268044\n",
      "4  near_high_speed_vehicle  static  0.266938\n",
      "================================================================================\n",
      "QUERY: The ego vehicle is entering a roundabout with multiple vehicles already inside.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory updated. New memory index size: 2\n",
      "\n",
      "Generated plan:\n",
      " The vehicle will follow a green light. The vehicle will start to turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will continue to turn right and then turn left. The vehicle will continue to move forward. | base_scenario_type: near_high_speed_vehicle | base_scenario_id: 5195aa894d2b5952\n",
      "2. [source=static] type=stationary_in_traffic | text=Scenario type: stationary_in_traffic | Lidar token: b'\\xc4X!\\x9a\\xba\\xbaW\\x18' | Scenario id: b'\\xf4\\xf0Y\\r\\xdaLR\\n'\n",
      "3. [source=static] type=stationary_in_traffic | text=Scenario type: stationary_in_traffic | Lidar token: b'\\x94\\x13\\x11\\r~CZg' | Scenario id: b'1\\x03\\x83Sg!\\\\\\xa4'\n",
      "\n",
      "Current query / situation:\n",
      "The ego vehicle is entering a roundabout with multiple vehicles already inside.\n",
      "\n",
      "Based on the retrieved scenarios, describe a safe high-level plan for the ego vehicle in 2-3 sentences.\n",
      "\n",
      "Plan: The vehicle will go to the left or right on the right side.\n",
      "\n",
      "Plan: The vehicle will turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will turn right and\n",
      "\n",
      "Top retrieved sources:\n",
      "              scenario_type  source     score\n",
      "0  near_high_speed_vehicle  memory  0.525019\n",
      "1    stationary_in_traffic  static  0.311187\n",
      "2    stationary_in_traffic  static  0.304786\n",
      "3    stationary_in_traffic  static  0.301709\n",
      "4    stationary_in_traffic  static  0.300261\n",
      "================================================================================\n",
      "QUERY: A pedestrian starts crossing unexpectedly at a crosswalk while the ego car is braking.\n",
      "✅ Memory updated. New memory index size: 3\n",
      "\n",
      "Generated plan:\n",
      " The vehicle will follow a green light. The vehicle will start to turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will continue to turn right and then turn left. The vehicle will continue to move forward. | base_scenario_type: near_high_speed_vehicle | base_scenario_id: 5195aa894d2b5952\n",
      "2. [source=static] type=near_pedestrian_on_crosswalk | text=Scenario type: near_pedestrian_on_crosswalk | Lidar token: b'\\xd0$\\xf8\\xab\\xd3\\x0f[\\xaa' | Scenario id: b'\\xae\\x89\\x02\\x12\\x05-[a'\n",
      "3. [source=static] type=near_pedestrian_on_crosswalk | text=Scenario type: near_pedestrian_on_crosswalk | Lidar token: b'\\xd0$\\xf8\\xab\\xd3\\x0f[\\xaa' | Scenario id: b'AvZ\\x13\\x9a-Y\\xc8'\n",
      "\n",
      "Current query / situation:\n",
      "A pedestrian starts crossing unexpectedly at a crosswalk while the ego car is braking.\n",
      "\n",
      "Based on the retrieved scenarios, describe a safe high-level plan for the ego vehicle in 2-3 sentences.\n",
      "\n",
      "Plan: The ego vehicle is approaching a green light. The vehicle will follow a green light.\n",
      "\n",
      "Plan: The vehicle will turn right and then turn left. The vehicle will continue to move forward.\n",
      "\n",
      "Plan: The vehicle will continue to turn right and then turn left. The vehicle will continue to move forward. | base_scenario_type: near_pedestrian_on_cross\n",
      "\n",
      "Top retrieved sources:\n",
      "                   scenario_type  source     score\n",
      "0       near_high_speed_vehicle  memory  0.514657\n",
      "1  near_pedestrian_on_crosswalk  static  0.413391\n",
      "2  near_pedestrian_on_crosswalk  static  0.412690\n",
      "3  near_pedestrian_on_crosswalk  static  0.403056\n",
      "4  near_pedestrian_on_crosswalk  static  0.401613\n"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"The ego vehicle is approaching a red light with a slow lead car ahead.\",\n",
    "    \"The ego vehicle is entering a roundabout with multiple vehicles already inside.\",\n",
    "    \"A pedestrian starts crossing unexpectedly at a crosswalk while the ego car is braking.\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for q in example_queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"QUERY:\", q)\n",
    "    out = plan_with_memory(q, k_static=5, k_memory=5, top_m_for_llm=3, update_memory_flag=True)\n",
    "    results.append(out)\n",
    "    print(\"\\nGenerated plan:\\n\", out[\"plan\"])\n",
    "    print(\"\\nTop retrieved sources:\\n\", out[\"retrieved\"][[\"scenario_type\", \"source\", \"score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1f809-c88a-458c-bf9e-7243069375b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
